{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuUZL6RWlx0G"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "hRd701FZlx0H"
      },
      "outputs": [],
      "source": [
        "# Fix randomness and hide warnings\n",
        "RND = False\n",
        "if not RND:\n",
        "    seed = 76998669\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "if not RND:\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "os.environ['MPLCONFIGDIR'] = os.getcwd()+'/configs/'\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "\n",
        "import numpy as np\n",
        "if not RND:\n",
        "    np.random.seed(seed)\n",
        "\n",
        "import logging\n",
        "\n",
        "import random\n",
        "if not RND:\n",
        "    random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Wmj58MClx0J",
        "outputId": "e8ec505a-a1c6-4828-dd0a-d913939633f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.14.0\n"
          ]
        }
      ],
      "source": [
        "# Import tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras as tfk\n",
        "from keras import layers as tfkl\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "tf.autograph.set_verbosity(0)\n",
        "tf.get_logger().setLevel(logging.ERROR)\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "if not RND:\n",
        "    tf.random.set_seed(seed)\n",
        "    tf.compat.v1.set_random_seed(seed)\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "VE4794-1lx0J"
      },
      "outputs": [],
      "source": [
        "# Import other libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZf7w3eUlx0J"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ku4r3gXdlx0J",
        "outputId": "c33631f7-8188-474d-eb6f-e09087844e3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-06 15:57:49--  https://storage.googleapis.com/storage.barbiero.dev/public_data_no_meme.npz\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.2.207, 2607:f8b0:4023:c0d::cf, 2607:f8b0:4023:c0b::cf\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.2.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 553413073 (528M) [application/octet-stream]\n",
            "Saving to: ‘public_data_no_meme.npz.1’\n",
            "\n",
            "public_data_no_meme 100%[===================>] 527.78M  26.6MB/s    in 21s     \n",
            "\n",
            "2023-11-06 15:58:11 (25.1 MB/s) - ‘public_data_no_meme.npz.1’ saved [553413073/553413073]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# download clean dataset\n",
        "!wget https://storage.googleapis.com/storage.barbiero.dev/public_data_no_meme.npz\n",
        "\n",
        "# load dataset\n",
        "dataset = np.load('public_data_no_meme.npz', allow_pickle=True)\n",
        "keys = list(dataset.keys())\n",
        "images = np.array(dataset[keys[0]])\n",
        "labels = np.array(dataset[keys[1]])\n",
        "\n",
        "labels_map = {0: \"healthy\", 1: \"unhealthy\"}\n",
        "labels_rev_map = {\"healthy\": 0, \"unhealthy\": 1}\n",
        "labels = np.array([labels_rev_map[label] for label in labels])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJEEkS3olx0K"
      },
      "source": [
        "## Split data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "xU-n7kCWlx0K"
      },
      "outputs": [],
      "source": [
        "# Split the dataset into a combined training and validation set, and a separate test set\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    images,\n",
        "    labels,\n",
        "    test_size = int(0.15 * len(images)),\n",
        "    **({\"random_state\":seed} if not RND else {}),\n",
        "    stratify = labels\n",
        ")\n",
        "\n",
        "# Further split the combined training and validation set into a training set and a validation set\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val,\n",
        "    y_train_val,\n",
        "    test_size = int(0.15 * len(images)),\n",
        "    **({\"random_state\":seed} if not RND else {}),\n",
        "    stratify = y_train_val\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,)\n",
        "\n",
        "gen_images = 1 # Number of images that has to be generated\n",
        "for img in datagen.flow(X_train,y_train,batch_size = 1):\n",
        "  if gen_images <= 0:\n",
        "    break\n",
        "  gen_images -= 1\n",
        "  X_train = np.insert(X_train,0,img[0][0],axis=0)\n",
        "  y_train = np.insert(y_train,0,img[1][0],axis=0)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4J__UU21l8NB",
        "outputId": "fbd032ae-e129-45a7-f9a2-71f6094aa487"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[136.76894  140.32086  120.35193 ]\n",
            "  [154.01665  156.19038  142.10973 ]\n",
            "  [156.39185  157.21712  147.42624 ]\n",
            "  ...\n",
            "  [119.       117.       102.      ]\n",
            "  [119.       117.       102.      ]\n",
            "  [119.       117.       102.      ]]\n",
            "\n",
            " [[148.08308  150.69229  132.31007 ]\n",
            "  [157.00441  157.17299  145.09341 ]\n",
            "  [159.99237  160.03044  151.71364 ]\n",
            "  ...\n",
            "  [119.       117.       102.      ]\n",
            "  [119.       117.       102.      ]\n",
            "  [119.       117.       102.      ]]\n",
            "\n",
            " [[143.48462  144.20837  127.718666]\n",
            "  [152.95271  152.95271  141.65709 ]\n",
            "  [162.86713  162.76665  155.31047 ]\n",
            "  ...\n",
            "  [119.       117.       102.      ]\n",
            "  [119.       117.       102.      ]\n",
            "  [119.       117.       102.      ]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 45.338524  59.507786  24.84631 ]\n",
            "  [ 46.906357  61.906357  28.906359]\n",
            "  [ 45.891876  60.891876  27.927917]\n",
            "  ...\n",
            "  [ 69.27097  101.922585  33.61936 ]\n",
            "  [ 72.353676 104.83402   36.87332 ]\n",
            "  [ 75.43637  107.74546   40.127277]]\n",
            "\n",
            " [[ 45.68836   60.03254   25.7209  ]\n",
            "  [ 46.73144   61.73144   28.731441]\n",
            "  [ 44.842365  59.842365  27.228245]\n",
            "  ...\n",
            "  [ 64.84045   97.84045   27.613482]\n",
            "  [ 64.32666   97.32666   27.44222 ]\n",
            "  [ 63.81288   96.81288   27.27096 ]]\n",
            "\n",
            " [[ 46.038197  60.557293  26.595491]\n",
            "  [ 46.556522  61.556522  28.556522]\n",
            "  [ 43.79286   58.79286   26.528572]\n",
            "  ...\n",
            "  [ 73.479576 106.479576  35.479572]\n",
            "  [ 71.25318  104.25318   33.25318 ]\n",
            "  [ 69.02679  102.02679   31.026789]]]\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qv64m8jOlx0K"
      },
      "source": [
        "## Inspect data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lj9O_yOclx0K"
      },
      "outputs": [],
      "source": [
        "# Print the shapes of the resulting sets\n",
        "print('Training set shape:\\t',X_train.shape, y_train.shape)\n",
        "print('Validation set shape:\\t',X_val.shape, y_val.shape)\n",
        "print('Test set shape:\\t\\t',X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMyIjKL5lx0L"
      },
      "source": [
        "## Process data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoWRgZF6lx0L"
      },
      "outputs": [],
      "source": [
        "# Normalize data to the range [0, 1]\n",
        "X_train = X_train.astype(\"float32\")/255.\n",
        "X_val = X_val.astype(\"float32\")/255.\n",
        "X_test = X_test.astype(\"float32\")/255."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNdPK9Umlx0L"
      },
      "outputs": [],
      "source": [
        "# Display the count of occurrences of target classes in the training-validation dataset\n",
        "print('Counting occurrences of target classes:')\n",
        "print(pd.DataFrame(y_train, columns=['class'])['class'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDHJDIzJlx0L"
      },
      "outputs": [],
      "source": [
        "# Convert labels to categorical format using one-hot encoding\n",
        "y_train = tfk.utils.to_categorical(y_train,len(np.unique(y_train)))\n",
        "y_val = tfk.utils.to_categorical(y_val,len(np.unique(y_val)))\n",
        "y_test = tfk.utils.to_categorical(y_test,len(np.unique(y_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lbFSO5Llx0M"
      },
      "outputs": [],
      "source": [
        "print('Categorical label:', y_train[0])           # Display the categorical label\n",
        "print('\"Default\" label:', np.argmax(y_train[0]))   # Display the equivalent numeric label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpeuSBGUlx0M"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "we2zZBS5lx0M"
      },
      "outputs": [],
      "source": [
        "# Define key model parameters\n",
        "input_shape = X_train.shape[1:]  # Input shape for the model\n",
        "output_shape = y_train.shape[1]  # Output shape for the model\n",
        "batch_size = 128                # Batch size for training\n",
        "epochs = 200                     # Number of training epochs\n",
        "\n",
        "# Print the defined parameters\n",
        "print(\"Epochs:\", epochs)\n",
        "print(\"Batch Size:\", batch_size)\n",
        "print(\"Input Shape:\", input_shape)\n",
        "print(\"Output Shape:\", output_shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-Nhwi0glx0N"
      },
      "source": [
        "## Build model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvQCYv51lx0N"
      },
      "outputs": [],
      "source": [
        "# Model Function\n",
        "def apple_elixir_model(input_shape, output_shape):\n",
        "\n",
        "  preprocessing = tfk.Sequential([\n",
        "        tfkl.RandomBrightness(0.2, value_range=(0,1)),\n",
        "        tfkl.RandomTranslation(0.2,0.2),\n",
        "        tfkl.RandomZoom(0.2),\n",
        "        tfkl.RandomFlip(\"horizontal\"),\n",
        "        tfkl.RandomFlip(\"vertical\"),\n",
        "    ], name='preprocessing')\n",
        "\n",
        "  # Build the neural network layer by layer\n",
        "  input_layer = tfkl.Input(shape=input_shape, name='Input')\n",
        "\n",
        "  preprocessing = preprocessing(input_layer)\n",
        "\n",
        "  x = tfkl.Conv2D(filters=16, kernel_size=3,activation = 'relu')(preprocessing)\n",
        "  x = tfkl.MaxPooling2D()(x)\n",
        "\n",
        "  x = tfkl.Conv2D(filters=32,kernel_size=3,activation='relu')(preprocessing)\n",
        "  x = tfkl.Conv2D(filters=32,kernel_size=3,activation='relu')(x)\n",
        "  x = tfkl.MaxPooling2D()(x)\n",
        "\n",
        "  x1 = tfkl.Conv2D(filters=32, kernel_size=3,padding='same',activation = 'relu')(x)\n",
        "  x2 = tfkl.Conv2D(filters=32, kernel_size=3,padding='same', activation = 'relu')(x1)\n",
        "\n",
        "  x = tfkl.Add()([x,x2])\n",
        "  x = tfkl.ReLU()(x)\n",
        "  x = tfkl.MaxPooling2D()(x)\n",
        "\n",
        "  x = tfkl.Conv2D(filters=64,kernel_size=3,activation='relu')(x)\n",
        "  x = tfkl.MaxPooling2D()(x)\n",
        "\n",
        "\n",
        "  x1 = tfkl.Conv2D(filters=64,kernel_size=3,padding='same',activation='relu')(x)\n",
        "  x2 = tfkl.Conv2D(filters=64,kernel_size=3,padding='same',activation='relu')(x1)\n",
        "\n",
        "  x = tfkl.Add()([x,x2])\n",
        "  x = tfkl.ReLU()(x)\n",
        "  x = tfkl.MaxPooling2D()(x)\n",
        "\n",
        "  x = tfkl.Conv2D(filters=128,kernel_size=3,padding='same',activation='relu')(x)\n",
        "  x = tfkl.Conv2D(filters=128,kernel_size=3,padding='same',activation='relu')(x)\n",
        "  x = tfkl.GlobalAveragePooling2D()(x)\n",
        "\n",
        "  output_layer = tfkl.Dense(units=output_shape ,activation='softmax',name='Output')(x)\n",
        "\n",
        "  # Connect input and output through the Model class\n",
        "  model = tfk.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Nadam(weight_decay=5e-4), metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxoKrnpKlx0N"
      },
      "outputs": [],
      "source": [
        "model = apple_elixir_model(input_shape, output_shape)\n",
        "\n",
        "# Print the model summary and plot the model architecture\n",
        "model.summary()\n",
        "tfk.utils.plot_model(model, expand_nested=True, show_shapes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2ZpAPLQlx0N"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q15Wkq7Glx0N"
      },
      "outputs": [],
      "source": [
        "early_stopping = tfk.callbacks.EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=20,\n",
        "    mode='max',\n",
        "    restore_best_weights=True)\n",
        "\n",
        "lr_scheduler = tfk.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_accuracy',     # Metric to monitor (validation mean squared error in this case)\n",
        "    patience=7,  # Number of epochs with no improvement after which learning rate will be reduced\n",
        "    factor=0.90,          # Factor by which the learning rate will be reduced (0.999 in this case)\n",
        "    mode='max',            # Mode to decide when to reduce learning rate ('min' means reduce when metric stops decreasing)\n",
        "    min_lr=1e-7            # Minimum learning rate\n",
        ")\n",
        "\n",
        "\n",
        "callbacks = [early_stopping,lr_scheduler]\n",
        "\n",
        "# Train the model and save its history\n",
        "history = model.fit(\n",
        "    x=X_train,\n",
        "    y=y_train,\n",
        "    batch_size=32,\n",
        "    epochs=500,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=callbacks\n",
        ").history\n",
        "\n",
        "# Save the trained model\n",
        "model.save('CHANGE_THIS_NAME')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlLAX2dHlx0N"
      },
      "outputs": [],
      "source": [
        "# Find the epoch with the highest validation accuracy\n",
        "best_epoch = np.argmax(history['val_accuracy'])\n",
        "\n",
        "# Plot training and validation performance metrics\n",
        "plt.figure(figsize=(20, 5))\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.plot(history['loss'], label='Training', alpha=0.8, color='#ff7f0e', linewidth=3)\n",
        "plt.plot(history['val_loss'], label='Validation', alpha=0.8, color='#4D61E2', linewidth=3)\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('Binary Crossentropy')\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "plt.figure(figsize=(20, 5))\n",
        "\n",
        "# Plot training and validation accuracy, highlighting the best epoch\n",
        "plt.plot(history['accuracy'], label='Training', alpha=0.8, color='#ff7f0e', linewidth=3)\n",
        "plt.plot(history['val_accuracy'], label='Validation', alpha=0.8, color='#4D61E2', linewidth=3)\n",
        "plt.plot(best_epoch, history['val_accuracy'][best_epoch], marker='*', alpha=0.8, markersize=10, color='#4D61E2')\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('Accuracy')\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "test_predictions = model.predict(X_test, verbose=0)\n",
        "test_predictions = np.argmax(test_predictions, axis=-1)\n",
        "test_gt = np.argmax(y_test, axis=-1)\n",
        "test_accuracy = accuracy_score(test_gt, test_predictions)\n",
        "print(f'Accuracy Score over the Test Set: {round(test_accuracy, 4)}')\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/My Drive/AN2DL\n",
        "\n",
        "model.save('AugmentedResidual')"
      ],
      "metadata": {
        "id": "9HHCVSScwN0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> larger filters\n",
        "\n",
        "> adam optimizer\n",
        "\n",
        "\n",
        "> scheduled learning rate in training seems to be effective\n",
        "\n",
        "\n",
        "> lot of augmentation\n",
        "\n",
        "\n",
        "\n",
        "> residual learning seems to be effective"
      ],
      "metadata": {
        "id": "aHIGJ9d-xJfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "# Compute the confusion matrix\n",
        "test_predictions = model.predict(X_test, verbose=0)\n",
        "cm = confusion_matrix(np.argmax(y_test, axis=-1), np.argmax(test_predictions, axis=-1))\n",
        "\n",
        "# Compute classification metrics\n",
        "accuracy = accuracy_score(np.argmax(y_test, axis=-1), np.argmax(test_predictions, axis=-1))\n",
        "precision = precision_score(np.argmax(y_test, axis=-1), np.argmax(test_predictions, axis=-1), average='macro')\n",
        "recall = recall_score(np.argmax(y_test, axis=-1), np.argmax(test_predictions, axis=-1), average='macro')\n",
        "f1 = f1_score(np.argmax(y_test, axis=-1), np.argmax(test_predictions, axis=-1), average='macro')\n",
        "\n",
        "# Display the computed metrics\n",
        "print('Accuracy:', accuracy.round(4))\n",
        "print('Precision:', precision.round(4))\n",
        "print('Recall:', recall.round(4))\n",
        "print('F1:', f1.round(4))\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm.T, cmap='Blues')\n",
        "plt.xlabel('True labels')\n",
        "plt.ylabel('Predicted labels')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4JpfEFdqxAjh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}