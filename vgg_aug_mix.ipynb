{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import logging\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as tfk\n",
    "from tensorflow.keras import layers as tfkl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Fix randomness and hide warnings\n",
    "RND = False\n",
    "if not RND:\n",
    "    seed = 76998669\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "if not RND:\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd()+'/configs/'\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "if not RND:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "tf.autograph.set_verbosity(0)\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "if not RND:\n",
    "    tf.random.set_seed(seed)\n",
    "    tf.compat.v1.set_random_seed(seed)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download clean dataset\n",
    "!wget https://storage.googleapis.com/storage.barbiero.dev/public_data_clean.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "dataset = np.load('public_data_clean.npz', allow_pickle=True)\n",
    "keys = list(dataset.keys())\n",
    "images = np.array(dataset[keys[0]])\n",
    "labels = np.array(dataset[keys[1]])\n",
    "\n",
    "labels_map = {0: \"healthy\", 1: \"unhealthy\"}\n",
    "labels_rev_map = {\"healthy\": 0, \"unhealthy\": 1}\n",
    "labels = np.array([labels_rev_map[label] for label in labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into a combined training and validation set, and a separate test set\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    images,\n",
    "    labels,\n",
    "    test_size = 0.1,\n",
    "    **({\"random_state\":seed} if not RND else {}),\n",
    "    stratify = labels\n",
    ")\n",
    "\n",
    "# Further split the combined training and validation set into a training set and a validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val,\n",
    "    y_train_val,\n",
    "    test_size = 0.2,\n",
    "    **({\"random_state\":seed} if not RND else {}),\n",
    "    stratify = y_train_val\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment train set, rotation, flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment training set with all rotations and flips\n",
    "for i in range(len(X_train)):\n",
    "    # for all rotations\n",
    "    for j in range(3):\n",
    "        # rotate image\n",
    "        X_train = np.append(X_train, [np.rot90(X_train[i], j + 1)], axis=0)\n",
    "    \n",
    "    flipped = np.fliplr(X_train[i])\n",
    "    # flip image\n",
    "    X_train = np.append(X_train, [flipped], axis=0)\n",
    "    # for all rotations\n",
    "    for j in range(3):\n",
    "        # rotate image\n",
    "        X_train = np.append(X_train, [np.rot90(flipped, j + 1)], axis=0)\n",
    "    # add 7 labels\n",
    "    y_train = np.append(y_train, [y_train[i]] * 7, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shapes of the resulting sets\n",
    "print('Training set shape:\\t',X_train.shape, y_train.shape)\n",
    "print('Validation set shape:\\t',X_val.shape, y_val.shape)\n",
    "print('Test set shape:\\t\\t',X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a sample of images from the training-validation dataset\n",
    "num_img = 10\n",
    "fig, axes = plt.subplots(1, num_img, figsize=(96, 96))\n",
    "\n",
    "# Iterate through the selected number of images\n",
    "for i in range(num_img):\n",
    "    ax = axes[i % num_img]\n",
    "    ax.imshow(X_train_val[i]/255, cmap='gray')\n",
    "    ax.set_title(f'{labels_map[y_train_val[i]]}', fontsize=40)  # Show the corresponding label\n",
    "\n",
    "# Adjust layout and display the images\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data to the range [0, 1]\n",
    "X_train = X_train.astype(\"float32\")/255.\n",
    "X_val = X_val.astype(\"float32\")/255.\n",
    "X_test = X_test.astype(\"float32\")/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the count of occurrences of target classes in the training-validation dataset\n",
    "print('Counting occurrences of target classes:')\n",
    "print(pd.DataFrame(y_train, columns=['class'])['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to categorical format using one-hot encoding\n",
    "y_train = tfk.utils.to_categorical(y_train,len(np.unique(y_train)))\n",
    "y_val = tfk.utils.to_categorical(y_val,len(np.unique(y_val)))\n",
    "y_test = tfk.utils.to_categorical(y_test,len(np.unique(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Categorical label:', y_train[0])           # Display the categorical label\n",
    "print('\"Default\" label:', np.argmax(y_train[0]))   # Display the equivalent numeric label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define key model parameters\n",
    "input_shape = X_train.shape[1:]  # Input shape for the model\n",
    "output_shape = y_train.shape[1]  # Output shape for the model\n",
    "batch_size = 64                  # Batch size for training\n",
    "epochs = 200                     # Number of training epochs\n",
    "\n",
    "# Print the defined parameters\n",
    "print(\"Epochs:\", epochs)\n",
    "print(\"Batch Size:\", batch_size)\n",
    "print(\"Input Shape:\", input_shape)\n",
    "print(\"Output Shape:\", output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a residual convolutional block with optional batch normalization\n",
    "def conv_residual_block(x, filters, kernel_size, padding='same', downsample=True, activation='relu', stack=2, batch_norm=True, name=''):\n",
    "\n",
    "    # If downsample is True, apply max-pooling\n",
    "    if downsample:\n",
    "        x = tfkl.MaxPooling2D(name='MaxPool_' + name)(x)\n",
    "\n",
    "    # Create a copy of the input for the residual connection\n",
    "    x_ = x\n",
    "\n",
    "    # Apply a stack of convolutional layers to the copy\n",
    "    for s in range(stack):\n",
    "        x_ = tfkl.Conv2D(filters=filters, kernel_size=kernel_size, padding=padding, name='Conv_' + name + str(s+1))(x_)\n",
    "        if batch_norm:\n",
    "            x_ = tfkl.BatchNormalization(name='BatchNorm_' + name + str(s+1))(x_)\n",
    "        x_ = tfkl.Activation(activation, name='Activation_' + name + str(s+1))(x_)\n",
    "\n",
    "    # If downsample is True, apply a 1x1 convolution to match the number of filters\n",
    "    if downsample:\n",
    "        x = tfkl.Conv2D(filters=filters, kernel_size=1, padding=padding, name='Conv_' + name + 'skip')(x)\n",
    "\n",
    "    # Add the original and the processed copy to create the residual connection\n",
    "    x = tfkl.Add(name='Add_' + name)([x_, x])\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Function\n",
    "def apple_elixir_model(input_shape, output_shape):\n",
    "  \n",
    "  # Input layer\n",
    "    input_layer = tfkl.Input(shape=input_shape, name='Input_Layer')\n",
    "\n",
    "    # Define a preprocessing Sequential model with random flip, zero padding, and random crop\n",
    "    preprocessing = tfk.Sequential([\n",
    "        tfkl.RandomFlip('horizontal', name='RandomFlip_horizontal'),\n",
    "        tfkl.RandomRotation(0.2, name='RandomRotation'),\n",
    "        \n",
    "        #tfkl.ZeroPadding2D((2, 2), name='ZeroPadding_2x2'),\n",
    "        #tfkl.RandomCrop(input_shape[0], input_shape[1], name='RandomCrop')\n",
    "    ], name='Preprocessing')\n",
    "\n",
    "    # Apply preprocessing to the input layer\n",
    "    x0 = preprocessing(input_layer)\n",
    "\n",
    "    # Initial convolution with batch normalization and activation\n",
    "    x0 = tfkl.Conv2D(filters=64, kernel_size=3, padding='same', name='Conv0')(x0)\n",
    "    x0 = tfkl.BatchNormalization(name='BatchNorm0')(x0)\n",
    "    x0 = tfkl.Activation('relu', name='ReLU0')(x0)\n",
    "\n",
    "    # Create residual blocks\n",
    "    x1 = conv_residual_block(x=x0, filters=64, kernel_size=3, downsample=False, stack=2, name='1')\n",
    "    x1 = conv_residual_block(x=x1, filters=64, kernel_size=3, downsample=False, stack=2, name='2')\n",
    "\n",
    "    x2 = conv_residual_block(x=x1, filters=128, kernel_size=3, downsample=True, stack=2, name='3')\n",
    "    x2 = conv_residual_block(x=x2, filters=128, kernel_size=3, downsample=False, stack=2, name='4')\n",
    "\n",
    "    x3 = conv_residual_block(x=x2, filters=256, kernel_size=3, downsample=True, stack=2, name='5')\n",
    "    x3 = conv_residual_block(x=x3, filters=256, kernel_size=3, downsample=False, stack=2, name='6')\n",
    "\n",
    "    x4 = conv_residual_block(x=x3, filters=512, kernel_size=3, downsample=True, stack=2, name='7')\n",
    "    x4 = conv_residual_block(x=x4, filters=512, kernel_size=3, downsample=False, stack=2, name='8')\n",
    "\n",
    "    # Global Average Pooling and classifier\n",
    "    x = tfkl.GlobalAveragePooling2D(name='GlobalAveragePooling')(x4)\n",
    "    x = tfkl.Dropout(.3, name='dropout1')(x)\n",
    "\n",
    "    x = tfkl.Dense(64, name='classifier')(x)\n",
    "    x = tfkl.Dropout(.3, name='dropout2')(x)\n",
    "\n",
    "    x = tfkl.Dense(output_shape, name='output')(x)\n",
    "    output_activation = tfkl.Activation('softmax', name='Softmax')(x)\n",
    "\n",
    "    # Create the model\n",
    "    model = tfk.Model(inputs=input_layer, outputs=output_activation, name='VGG18_Residual')\n",
    "\n",
    "    # Define optimizer, loss, and metrics\n",
    "    # AdamW is an Adam optimizer which applies weight_decay to network layers,\n",
    "    # i.e it's another way to apply l2 regularization to the whole network\n",
    "    optimizer = tfk.optimizers.AdamW(1e-4, weight_decay=5e-4)\n",
    "    loss = tfk.losses.CategoricalCrossentropy()\n",
    "    metrics = ['accuracy']\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = apple_elixir_model(input_shape, output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model summary and plot the model architecture\n",
    "model.summary()\n",
    "tfk.utils.plot_model(model, expand_nested=True, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "early_stopping = tfk.callbacks.EarlyStopping(monitor='val_accuracy', patience=20, mode='max', restore_best_weights=True)\n",
    "\n",
    "callbacks = [early_stopping]\n",
    "\n",
    "# Train the model and save its history\n",
    "history = model.fit(\n",
    "    x=X_train,\n",
    "    y=y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks\n",
    ").history\n",
    "\n",
    "# Save the trained model\n",
    "model.save('CHANGE_THIS_NAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the epoch with the highest validation accuracy\n",
    "best_epoch = np.argmax(history['val_accuracy'])\n",
    "\n",
    "# Plot training and validation performance metrics\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.plot(history['loss'], label='Training', alpha=0.8, color='#ff7f0e', linewidth=3)\n",
    "plt.plot(history['val_loss'], label='Validation', alpha=0.8, color='#4D61E2', linewidth=3)\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Binary Crossentropy')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "# Plot training and validation accuracy, highlighting the best epoch\n",
    "plt.plot(history['accuracy'], label='Training', alpha=0.8, color='#ff7f0e', linewidth=3)\n",
    "plt.plot(history['val_accuracy'], label='Validation', alpha=0.8, color='#4D61E2', linewidth=3)\n",
    "plt.plot(best_epoch, history['val_accuracy'][best_epoch], marker='*', alpha=0.8, markersize=10, color='#4D61E2')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Accuracy')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Intermediate Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
