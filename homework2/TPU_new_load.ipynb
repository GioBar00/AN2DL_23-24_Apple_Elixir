{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6I2gGs9RRH4S"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/MyDrive/AN2DL/Homework2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ue6k86U2tWjc"
      },
      "outputs": [],
      "source": [
        "!wget -q https://storage.googleapis.com/storage.barbiero.dev/AN2DL/Homework_2/training_data_clean.npy\n",
        "!wget -q https://storage.googleapis.com/storage.barbiero.dev/AN2DL/Homework_2/categories_clean.npy\n",
        "!wget -q https://storage.googleapis.com/storage.barbiero.dev/AN2DL/Homework_2/series_length_clean.npy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbxwcBFJRUqk"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade tensorflow -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "p27kgQkaRdP3"
      },
      "outputs": [],
      "source": [
        "# Fix randomness and hide warnings\n",
        "seed = 42\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(seed)\n",
        "\n",
        "import logging\n",
        "\n",
        "import random\n",
        "\n",
        "random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQI_C2vzRq0s"
      },
      "outputs": [],
      "source": [
        "# Import tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras as tfk\n",
        "from tensorflow.keras import layers as tfkl\n",
        "\n",
        "tf.autograph.set_verbosity(0)\n",
        "tf.get_logger().setLevel(logging.ERROR)\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "tf.random.set_seed(seed)\n",
        "tf.compat.v1.set_random_seed(seed)\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "znP3lccERsTm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rc('font', size=16)\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vhmoI93WRd6Z"
      },
      "outputs": [],
      "source": [
        "use_tpu = True  #@param {type:\"boolean\"}\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ and use_tpu:\n",
        "    TF_MASTER = 'grpc://{}'.format(os.environ['COLAB_TPU_ADDR'])\n",
        "else:\n",
        "    use_tpu = False\n",
        "\n",
        "if use_tpu:\n",
        "    tpu_address = TF_MASTER\n",
        "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(TF_MASTER)\n",
        "    tf.config.experimental_connect_to_cluster(resolver)\n",
        "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "    strategy = tf.distribute.TPUStrategy(resolver)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nGgqZc8SRx_4"
      },
      "outputs": [],
      "source": [
        "def print_sequence(dataset, to_plot):\n",
        "    figs, axs = plt.subplots(to_plot, 1, sharex=True, figsize=(17, 17))\n",
        "    n_elements = len(dataset)\n",
        "    for i in range(to_plot):\n",
        "        el = random.randrange(n_elements)\n",
        "        axs[i].plot(dataset[el])\n",
        "        axs[i].set_title(el)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BHWONb8ASGh2"
      },
      "outputs": [],
      "source": [
        "def build_sequence(data, categories, window_size=200, stride=200, telescope=18):\n",
        "    actual_window = window_size + telescope\n",
        "\n",
        "    new_categories = []\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    for i, element in enumerate(data):\n",
        "        length = len(element)\n",
        "        # number of windows\n",
        "        n_windows = int(np.ceil((length - actual_window) / stride)) + 1\n",
        "        # reevaluate the stride\n",
        "        if n_windows > 1:\n",
        "            stride = int((length - actual_window) / (n_windows - 1)) + 1\n",
        "        # start from the end of the series\n",
        "        start_idx = length - actual_window\n",
        "        end_idx = length\n",
        "        # for each window\n",
        "        for j in range(n_windows):\n",
        "            if start_idx < 0:\n",
        "                start_idx = 0\n",
        "                end_idx = actual_window\n",
        "                if end_idx > length:\n",
        "                    end_idx = length\n",
        "            # append the window to X\n",
        "            temp = element[start_idx:end_idx - telescope]\n",
        "            if len(temp) < window_size:\n",
        "                temp = np.pad(temp, (0, window_size - len(temp)), mode=\"constant\", constant_values=-1)\n",
        "            X.append(temp)\n",
        "            # append the telescope to y\n",
        "            y.append(element[end_idx - telescope:end_idx])\n",
        "            # append the category\n",
        "            new_categories.append(categories[i])\n",
        "            # update the start and end index\n",
        "            start_idx -= stride\n",
        "            end_idx -= stride\n",
        "\n",
        "    return X, y, new_categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CnUKtAkRtXL"
      },
      "outputs": [],
      "source": [
        "training_data = np.load(\"training_data_clean.npy\", allow_pickle=True)\n",
        "training_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_y3wP2jRwVs"
      },
      "outputs": [],
      "source": [
        "print_sequence(training_data, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "DU53oCvFR5F4"
      },
      "outputs": [],
      "source": [
        "categories = np.load(\"categories_clean.npy\", allow_pickle=True)\n",
        "series_lengths = np.load(\"series_length_clean.npy\", allow_pickle=True)\n",
        "categories_Set = set(categories)\n",
        "\n",
        "dataset = [training_data[i, :series_lengths[i]] for i in range(len(training_data))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APfQNwZRR-w_"
      },
      "outputs": [],
      "source": [
        "print_sequence(dataset, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "dmA-N-yztWjr"
      },
      "outputs": [],
      "source": [
        "window = 200\n",
        "stride = 200\n",
        "telescope = 18\n",
        "autoregressive_telescope = 18\n",
        "X, y, categories = build_sequence(dataset, categories, window_size=200, stride=200, telescope=18)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSGpCInuSD79"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train_val, X_test, y_train_val, y_test, categories_train_val, categories_test = train_test_split(X, y, categories,\n",
        "                                                                                       test_size=0.15,\n",
        "                                                                                       stratify=categories)\n",
        "X_train, X_val, y_train, y_val, categories_train, categories_val = train_test_split(X_train_val, y_train_val, categories_train_val,\n",
        "                                                                                       test_size=0.15,\n",
        "                                                                                       stratify=categories_train_val)\n",
        "print(len(X_train))\n",
        "print(len(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6joWHqLlGlCw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "04Zdoe5ISLOY"
      },
      "outputs": [],
      "source": [
        "X_train = np.expand_dims(X_train, axis=-1)\n",
        "X_val = np.expand_dims(X_val, axis=-1)\n",
        "X_test = np.expand_dims(X_test, axis=-1)\n",
        "y_train = np.expand_dims(y_train, axis=-1)\n",
        "y_val = np.expand_dims(y_val, axis=-1)\n",
        "y_test = np.expand_dims(y_test, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)"
      ],
      "metadata": {
        "id": "RT0Kv_-Suy3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "lebYxwZdSnXt"
      },
      "outputs": [],
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ctVXzm4rfqZT"
      },
      "outputs": [],
      "source": [
        "# Assign the batch size\n",
        "if use_tpu:\n",
        "    BATCH_SIZE = 64\n",
        "else:\n",
        "    BATCH_SIZE = 64\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Pa3ztpPLTOQR"
      },
      "outputs": [],
      "source": [
        "def prepare_trainset(dataset):\n",
        "    return (\n",
        "        dataset\n",
        "        .cache()\n",
        "        .shuffle(1000, reshuffle_each_iteration=True)\n",
        "        .repeat()\n",
        "        .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    )\n",
        "\n",
        "\n",
        "def prepare_valset(dataset):\n",
        "    return (\n",
        "        dataset\n",
        "        .cache()\n",
        "        .repeat()\n",
        "        .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    )\n",
        "\n",
        "\n",
        "train_dataset = prepare_trainset(train_dataset)\n",
        "val_dataset = prepare_valset(val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "y6cRtPAeSMPs"
      },
      "outputs": [],
      "source": [
        "input_shape = X_train.shape[1:]\n",
        "output_shape = y_train.shape[1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "dpHu8InPxwrt"
      },
      "outputs": [],
      "source": [
        "def build_CONV_LSTM_model(input_shape, output_shape):\n",
        "    # Ensure the input time steps are at least as many as the output time steps\n",
        "    assert input_shape[0] >= output_shape[0], \"For this exercise we want input time steps to be >= of output time steps\"\n",
        "\n",
        "    # Define the input layer with the specified shape\n",
        "    input_layer = tfkl.Input(shape=input_shape, name='input_layer')\n",
        "\n",
        "    x = tfkl.Masking(mask_value=-1)(input_layer)\n",
        "    # Add a Bidirectional LSTM layer with 64 units\n",
        "    x = tfkl.Bidirectional(tfkl.LSTM(128, return_sequences=True, name='lstm'), name='bidirectional_lstm')(x)\n",
        "\n",
        "    x = tfkl.Bidirectional(tfkl.LSTM(256, return_sequences=True, name='lstm'), name='bidirectional_lstm_2')(x)\n",
        "\n",
        "    # Add a 1D Convolution layer with 128 filters and a kernel size of 3\n",
        "    x = tfkl.Conv1D(128, 3, padding='same', activation='relu', name='conv')(x)\n",
        "\n",
        "    # Add a final Convolution layer to match the desired output shape\n",
        "    output_layer = tfkl.Conv1D(output_shape[1], 3, padding='same', name='output_layer')(x)\n",
        "\n",
        "    # Calculate the size to crop from the output to match the output shape\n",
        "    crop_size = output_layer.shape[1] - output_shape[0]\n",
        "    print(output_layer.shape[1])\n",
        "\n",
        "    # Crop the output to the desired length\n",
        "    output_layer = tfkl.Cropping1D((0, crop_size), name='cropping')(output_layer)\n",
        "\n",
        "    # Construct the model by connecting input and output layers\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer, name='CONV_LSTM_model')\n",
        "\n",
        "    # Compile the model with Mean Squared Error loss and Adam optimizer\n",
        "    model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=tf.keras.optimizers.Adam())\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lU7lHZu-TBm-"
      },
      "outputs": [],
      "source": [
        "if use_tpu:\n",
        "    with strategy.scope():\n",
        "        model = build_CONV_LSTM_model(input_shape, output_shape)\n",
        "else:\n",
        "    model = build_CONV_LSTM_model(input_shape, output_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Z6ZqbeKSP-N"
      },
      "outputs": [],
      "source": [
        "model.summary()\n",
        "tfk.utils.plot_model(model, expand_nested=True, show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8rS4_znSRXo"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    steps_per_epoch=len(X_train) // BATCH_SIZE,\n",
        "    epochs=300,\n",
        "    validation_data=val_dataset,\n",
        "    validation_steps=len(X_val) // BATCH_SIZE,\n",
        "    callbacks=[\n",
        "        tfk.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=20, restore_best_weights=True,\n",
        "                                    min_delta=0.0005),\n",
        "        tfk.callbacks.ReduceLROnPlateau(monitor='val_loss', mode='min', patience=10, factor=0.5, min_lr=1e-5)\n",
        "    ]\n",
        ").history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ba4ufQpMSb4d"
      },
      "outputs": [],
      "source": [
        "best_epoch = np.argmin(history['val_loss'])\n",
        "plt.figure(figsize=(17, 4))\n",
        "plt.plot(history['loss'], label='Training loss', alpha=.8, color='#ff7f0e')\n",
        "plt.plot(history['val_loss'], label='Validation loss', alpha=.9, color='#5a9aa5')\n",
        "plt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\n",
        "plt.title('Mean Squared Error')\n",
        "plt.legend()\n",
        "plt.grid(alpha=.3)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(18, 3))\n",
        "plt.plot(history['lr'], label='Learning Rate', alpha=.8, color='#ff7f0e')\n",
        "plt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\n",
        "plt.legend()\n",
        "plt.grid(alpha=.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAMP1Pmhw5Uw"
      },
      "outputs": [],
      "source": [
        "reg_predictions = np.array([])\n",
        "X_temp = X_test\n",
        "for reg in range(0, telescope, autoregressive_telescope):\n",
        "    pred_temp = model.predict(X_temp, verbose=0)\n",
        "    if (len(reg_predictions) == 0):\n",
        "        reg_predictions = pred_temp\n",
        "    else:\n",
        "        reg_predictions = np.concatenate((reg_predictions, pred_temp), axis=1)\n",
        "    X_temp = np.concatenate((X_temp[:, autoregressive_telescope:, :], pred_temp), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pcnuO4hxEhU"
      },
      "outputs": [],
      "source": [
        "# Print the shape of the predictions\n",
        "print(f\"Predictions shape: {reg_predictions.shape}\")\n",
        "\n",
        "print(\"Prediction at 18:\")\n",
        "# Calculate and print Mean Squared Error (MSE)\n",
        "mean_squared_error = tfk.metrics.mean_squared_error(y_test.flatten(), reg_predictions.flatten()).numpy()\n",
        "print(f\"Mean Squared Error: {mean_squared_error}\")\n",
        "\n",
        "# Calculate and print Mean Absolute Error (MAE)\n",
        "mean_absolute_error = tfk.metrics.mean_absolute_error(y_test.flatten(), reg_predictions.flatten()).numpy()\n",
        "print(f\"Mean Absolute Error: {mean_absolute_error}\")\n",
        "\n",
        "print(\"Prediction at 9:\")\n",
        "y_test_9 = y_test[:, :9]\n",
        "reg_predictions_9 = reg_predictions[:, :9]\n",
        "\n",
        "# Calculate and print Mean Squared Error (MSE)\n",
        "mean_squared_error = tfk.metrics.mean_squared_error(y_test_9.flatten(), reg_predictions_9.flatten()).numpy()\n",
        "print(f\"Mean Squared Error: {mean_squared_error}\")\n",
        "\n",
        "# Calculate and print Mean Absolute Error (MAE)\n",
        "mean_absolute_error = tfk.metrics.mean_absolute_error(y_test_9.flatten(), reg_predictions_9.flatten()).numpy()\n",
        "print(f\"Mean Absolute Error: {mean_absolute_error}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jo5aHPQP1Ab7"
      },
      "outputs": [],
      "source": [
        "def inspect_multivariate_prediction(X, y, pred, telescope, to_print):\n",
        "    figs, axs = plt.subplots(to_print, 1, sharex=True, figsize=(17, 17))\n",
        "    n_elements = X.shape[0]\n",
        "    for i in range(to_print):\n",
        "        el = random.randrange(n_elements)\n",
        "        axs[i].plot(np.arange(len(X[el, :, 0])), X[el, :, 0])\n",
        "        axs[i].plot(np.arange(len(X[el, :, 0]) - 1, len(X[el, :, 0]) + telescope - 1), y[el, :, 0], color='orange')\n",
        "        axs[i].plot(np.arange(len(X[el, :, 0]) - 1, len(X[el, :, 0]) + telescope - 1), pred[el, :], color='green')\n",
        "        axs[i].set_ylim(0, 1)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCEzVShxHcsD"
      },
      "outputs": [],
      "source": [
        "inspect_multivariate_prediction(X_test, y_test, reg_predictions, telescope, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hx8SNzcAT2r5"
      },
      "outputs": [],
      "source": [
        "NAME_MODEL = \"BidirectionalGRU\"\n",
        "if use_tpu:\n",
        "    # save model locally from tpu using Tensorflow's \"SavedModel\" format\n",
        "    save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n",
        "    model.save(NAME_MODEL, options=save_locally)\n",
        "else:\n",
        "    model.save(NAME_MODEL)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}