{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import logging\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as tfk\n",
    "from tensorflow.keras import layers as tfkl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Fix randomness and hide warnings\n",
    "RND = False\n",
    "if not RND:\n",
    "    seed = 76998669\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "if not RND:\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd()+'/configs/'\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "if not RND:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "tf.autograph.set_verbosity(0)\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "if not RND:\n",
    "    tf.random.set_seed(seed)\n",
    "    tf.compat.v1.set_random_seed(seed)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download clean dataset\n",
    "!wget https://apps.barbiero.dev/static/public_data_clean.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "dataset = np.load('public_data_clean.npz', allow_pickle=True)\n",
    "keys = list(dataset.keys())\n",
    "images = np.array(dataset[keys[0]])\n",
    "labels = np.array(dataset[keys[1]])\n",
    "\n",
    "labels_map = {0: \"healthy\", 1: \"unhealthy\"}\n",
    "labels_rev_map = {\"healthy\": 0, \"unhealthy\": 1}\n",
    "labels = np.array([labels_rev_map[label] for label in labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into a combined training and validation set, and a separate test set\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    images,\n",
    "    labels,\n",
    "    test_size = int(0.15 * len(images)),\n",
    "    **({\"random_state\":seed} if not RND else {}),\n",
    "    stratify = labels\n",
    ")\n",
    "\n",
    "# Further split the combined training and validation set into a training set and a validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val,\n",
    "    y_train_val,\n",
    "    test_size = int(0.15 * len(images)),\n",
    "    **({\"random_state\":seed} if not RND else {}),\n",
    "    stratify = y_train_val\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shapes of the resulting sets\n",
    "print('Training set shape:\\t',X_train.shape, y_train.shape)\n",
    "print('Validation set shape:\\t',X_val.shape, y_val.shape)\n",
    "print('Test set shape:\\t\\t',X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a sample of images from the training-validation dataset\n",
    "num_img = 10\n",
    "fig, axes = plt.subplots(1, num_img, figsize=(96, 96))\n",
    "\n",
    "# Iterate through the selected number of images\n",
    "for i in range(num_img):\n",
    "    ax = axes[i % num_img]\n",
    "    ax.imshow(X_train_val[i]/255, cmap='gray')\n",
    "    ax.set_title(f'{labels_map[y_train_val[i]]}', fontsize=40)  # Show the corresponding label\n",
    "\n",
    "# Adjust layout and display the images\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data to the range [0, 1]\n",
    "X_train = X_train.astype(\"float32\")/255.\n",
    "X_val = X_val.astype(\"float32\")/255.\n",
    "X_test = X_test.astype(\"float32\")/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the count of occurrences of target classes in the training-validation dataset\n",
    "print('Counting occurrences of target classes:')\n",
    "print(pd.DataFrame(y_train, columns=['class'])['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to categorical format using one-hot encoding\n",
    "y_train = tfk.utils.to_categorical(y_train,len(np.unique(y_train)))\n",
    "y_val = tfk.utils.to_categorical(y_val,len(np.unique(y_val)))\n",
    "y_test = tfk.utils.to_categorical(y_test,len(np.unique(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Categorical label:', y_train[0])           # Display the categorical label\n",
    "print('\"Default\" label:', np.argmax(y_train[0]))   # Display the equivalent numeric label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define key model parameters\n",
    "input_shape = X_train.shape[1:]  # Input shape for the model\n",
    "output_shape = y_train.shape[1]  # Output shape for the model\n",
    "batch_size = 128                # Batch size for training\n",
    "epochs = 200                     # Number of training epochs\n",
    "\n",
    "# Print the defined parameters\n",
    "print(\"Epochs:\", epochs)\n",
    "print(\"Batch Size:\", batch_size)\n",
    "print(\"Input Shape:\", input_shape)\n",
    "print(\"Output Shape:\", output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Function\n",
    "def apple_elixir_model(input_shape, output_shape):\n",
    "  \n",
    "  # Build the neural network layer by layer\n",
    "  input_layer = tfkl.Input(shape=input_shape, name='Input')\n",
    "\n",
    "  x = tfkl.Conv2D(filters=32,kernel_size=3,padding='same',activation='relu',name='conv00')(input_layer)\n",
    "  x = tfkl.Conv2D(filters=32,kernel_size=3,padding='same',activation='relu',name='conv01')(x)\n",
    "  x = tfkl.MaxPooling2D(name='mp0')(x)\n",
    "\n",
    "  x = tfkl.Conv2D(filters=64,kernel_size=3,padding='same',activation='relu',name='conv10')(x)\n",
    "  x = tfkl.Conv2D(filters=64,kernel_size=3,padding='same',activation='relu',name='conv11')(x)\n",
    "  x = tfkl.MaxPooling2D(name='mp1')(x)\n",
    "\n",
    "  x = tfkl.Conv2D(filters=128,kernel_size=3,padding='same',activation='relu',name='conv20')(x)\n",
    "  x = tfkl.Conv2D(filters=128,kernel_size=3,padding='same',activation='relu',name='conv21')(x)\n",
    "  x = tfkl.MaxPooling2D(name='mp2')(x)\n",
    "\n",
    "  x = tfkl.Conv2D(filters=256,kernel_size=3,padding='same',activation='relu',name='conv30')(x)\n",
    "  x = tfkl.Conv2D(filters=256,kernel_size=3,padding='same',activation='relu',name='conv31')(x)\n",
    "  x = tfkl.GlobalAveragePooling2D(name='gap')(x)\n",
    "\n",
    "  \n",
    "  x = tfkl.Dense(units = 30, activation='relu')(x)\n",
    "  x = tfkl.Dense(units = 20, activation='relu')(x)\n",
    "\n",
    "\n",
    "  output_layer = tfkl.Dense(units=output_shape ,activation='softmax',name='Output')(x)\n",
    "\n",
    "  # Connect input and output through the Model class\n",
    "  model = tfk.Model(inputs=input_layer, outputs=output_layer, name='Convnet')\n",
    "\n",
    "  # Compile the model\n",
    "  model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(weight_decay=5e-4), metrics=['accuracy'])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = apple_elixir_model(input_shape, output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model summary and plot the model architecture\n",
    "model.summary()\n",
    "tfk.utils.plot_model(model, expand_nested=True, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "early_stopping = tfk.callbacks.EarlyStopping(monitor='val_accuracy', patience=20, mode='max', restore_best_weights=True)\n",
    "\n",
    "callbacks = [early_stopping]\n",
    "\n",
    "# Train the model and save its history\n",
    "history = model.fit(\n",
    "    x=X_train,\n",
    "    y=y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks\n",
    ").history\n",
    "\n",
    "# Save the trained model\n",
    "model.save('CHANGE_THIS_NAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the epoch with the highest validation accuracy\n",
    "best_epoch = np.argmax(history['val_accuracy'])\n",
    "\n",
    "# Plot training and validation performance metrics\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.plot(history['loss'], label='Training', alpha=0.8, color='#ff7f0e', linewidth=3)\n",
    "plt.plot(history['val_loss'], label='Validation', alpha=0.8, color='#4D61E2', linewidth=3)\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Binary Crossentropy')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "# Plot training and validation accuracy, highlighting the best epoch\n",
    "plt.plot(history['accuracy'], label='Training', alpha=0.8, color='#ff7f0e', linewidth=3)\n",
    "plt.plot(history['val_accuracy'], label='Validation', alpha=0.8, color='#4D61E2', linewidth=3)\n",
    "plt.plot(best_epoch, history['val_accuracy'][best_epoch], marker='*', alpha=0.8, markersize=10, color='#4D61E2')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Accuracy')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Intermediate Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
